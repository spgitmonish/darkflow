{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from darkflow.net.build import TFNet\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from boxespredict.yolomodeltest import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the .pb file and the .meta file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading from .pb and .meta\n",
      "Running entirely on CPU\n"
     ]
    }
   ],
   "source": [
    "# The protocol buffer file and the .meta file\n",
    "# NOTE: The .met file is a JSON dump of everything necessary for post-processing such as anchors \n",
    "#       and labels\n",
    "if os.name == 'nt':\n",
    "    # 30000 steps(Model trained with Udacity Simulator Data)\n",
    "    #options = {\"pbLoad\": os.getcwd() + \"\\\\saved_graph\\\\30000-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"\\\\saved_graph\\\\30000-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1}\n",
    "    \n",
    "    # 40375 steps(Model trained with Udacity Simulator Data)\n",
    "    # options = {\"pbLoad\": os.getcwd() + \"\\\\saved_graph\\\\40375-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"\\\\saved_graph\\\\40375-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1}\n",
    "    \n",
    "    # 39000 steps(Model trained with Bosch and Udacity Simulator Data)\n",
    "    #options = {\"pbLoad\": os.getcwd() + \"\\\\saved_graph\\\\39000-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"\\\\saved_graph\\\\39000-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1}\n",
    "    \n",
    "    # 30000 steps(Model trained with Udacity Simulator and Real Data)\n",
    "    options = {\"pbLoad\": os.getcwd() + \"\\\\saved_graph\\\\30000-SimReal-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"\\\\saved_graph\\\\30000-SimReal-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1}\n",
    "else:\n",
    "    # 30000 steps(Model trained with Udacity Simulator Data)\n",
    "    #options = {\"pbLoad\": os.getcwd() + \"/saved_graph/30000-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"/saved_graph/30000-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1, \"gpu\": 1.0}\n",
    "\n",
    "    #40375 steps(Model trained with Udacity Simulator Data)\n",
    "    #options = {\"pbLoad\": os.getcwd() + \"/saved_graph/40375-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"/saved_graph/40375-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1, \"gpu\": 1.0}\n",
    "    \n",
    "    #39000 steps(Model trained with Bosch and Udacity Simulator Data)\n",
    "    #options = {\"pbLoad\": os.getcwd() + \"/saved_graph/39000-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"/saved_graph/39000-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1, \"gpu\": 1.0}\n",
    "    \n",
    "    #30000 steps(Model trained with Udacity Simulator and Real Data)\n",
    "    options = {\"pbLoad\": os.getcwd() + \"/saved_graph/30000-SimReal-tiny-yolo-voc-3c.pb\", \"metaLoad\": os.getcwd() + \"/saved_graph/30000-SimReal-tiny-yolo-voc-3c.meta\", \"threshold\": 0.1, \"gpu\": 1.0}\n",
    "\n",
    "# Object of Darkflow\n",
    "tfnet = TFNet(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the predictions from the model for all the images in the sample folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading from .pb and .meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\443615\\Documents\\SDC\\Term3\\SystemIntegration\\darkflow\\tf_light_darkflow_test\\boxespredict\\boxes.py:26: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  net_out = net_out_in.reshape([H, W, B, net_out_in.shape[2]/B])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'green1.jpg': [{'bottomright': {'x': 466, 'y': 219},\n",
      "                 'confidence': 0.99211675,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 340, 'y': 24}},\n",
      "                {'bottomright': {'x': 536, 'y': 295},\n",
      "                 'confidence': 0.13752569,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 264, 'y': 0}}],\n",
      " 'green2.jpg': [{'bottomright': {'x': 187, 'y': 397},\n",
      "                 'confidence': 0.99911457,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 112, 'y': 239}},\n",
      "                {'bottomright': {'x': 454, 'y': 389},\n",
      "                 'confidence': 0.95760024,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 387, 'y': 250}},\n",
      "                {'bottomright': {'x': 735, 'y': 402},\n",
      "                 'confidence': 0.99438053,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 666, 'y': 254}}],\n",
      " 'green3.jpg': [{'bottomright': {'x': 243, 'y': 457},\n",
      "                 'confidence': 0.97530389,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 188, 'y': 364}},\n",
      "                {'bottomright': {'x': 269, 'y': 503},\n",
      "                 'confidence': 0.13785161,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 173, 'y': 319}},\n",
      "                {'bottomright': {'x': 446, 'y': 465},\n",
      "                 'confidence': 0.97043794,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 393, 'y': 358}},\n",
      "                {'bottomright': {'x': 647, 'y': 474},\n",
      "                 'confidence': 0.97258788,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 596, 'y': 358}}],\n",
      " 'green4.jpg': [{'bottomright': {'x': 258, 'y': 462},\n",
      "                 'confidence': 0.99895334,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 218, 'y': 381}},\n",
      "                {'bottomright': {'x': 449, 'y': 475},\n",
      "                 'confidence': 0.9998678,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 402, 'y': 374}},\n",
      "                {'bottomright': {'x': 640, 'y': 462},\n",
      "                 'confidence': 0.99991632,\n",
      "                 'label': 'green_rect',\n",
      "                 'topleft': {'x': 596, 'y': 375}}],\n",
      " 'left0000.jpg': [{'bottomright': {'x': 660, 'y': 470},\n",
      "                   'confidence': 0.96450412,\n",
      "                   'label': 'green_rect',\n",
      "                   'topleft': {'x': 641, 'y': 410}},\n",
      "                  {'bottomright': {'x': 652, 'y': 464},\n",
      "                   'confidence': 0.35326609,\n",
      "                   'label': 'green_rect',\n",
      "                   'topleft': {'x': 632, 'y': 412}}],\n",
      " 'left0026.jpg': [],\n",
      " 'left0051.jpg': [],\n",
      " 'left0079.jpg': [{'bottomright': {'x': 652, 'y': 478},\n",
      "                   'confidence': 0.90218788,\n",
      "                   'label': 'orange_rect',\n",
      "                   'topleft': {'x': 632, 'y': 418}}],\n",
      " 'left0140.jpg': [{'bottomright': {'x': 558, 'y': 482},\n",
      "                   'confidence': 0.97846645,\n",
      "                   'label': 'red_rect',\n",
      "                   'topleft': {'x': 521, 'y': 387}}],\n",
      " 'left0572.jpg': [{'bottomright': {'x': 447, 'y': 512},\n",
      "                   'confidence': 0.9291712,\n",
      "                   'label': 'red_rect',\n",
      "                   'topleft': {'x': 396, 'y': 361}}],\n",
      " 'left0696.jpg': [{'bottomright': {'x': 323, 'y': 541},\n",
      "                   'confidence': 0.99876171,\n",
      "                   'label': 'green_rect',\n",
      "                   'topleft': {'x': 251, 'y': 349}}],\n",
      " 'left0710.jpg': [{'bottomright': {'x': 327, 'y': 534},\n",
      "                   'confidence': 0.99363643,\n",
      "                   'label': 'orange_rect',\n",
      "                   'topleft': {'x': 262, 'y': 346}}],\n",
      " 'nolight1.jpg': [],\n",
      " 'nolight2.jpg': [],\n",
      " 'nolight3.jpg': [],\n",
      " 'nolight4.jpg': [],\n",
      " 'orange1.jpg': [{'bottomright': {'x': 357, 'y': 226},\n",
      "                  'confidence': 0.95858669,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 238, 'y': 7}},\n",
      "                 {'bottomright': {'x': 799, 'y': 212},\n",
      "                  'confidence': 0.84279621,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 715, 'y': 32}}],\n",
      " 'orange2.jpg': [{'bottomright': {'x': 191, 'y': 406},\n",
      "                  'confidence': 0.99986625,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 120, 'y': 278}},\n",
      "                 {'bottomright': {'x': 445, 'y': 411},\n",
      "                  'confidence': 0.99704188,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 381, 'y': 282}},\n",
      "                 {'bottomright': {'x': 700, 'y': 429},\n",
      "                  'confidence': 0.99850017,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 637, 'y': 279}}],\n",
      " 'orange3.jpg': [{'bottomright': {'x': 402, 'y': 549},\n",
      "                  'confidence': 0.99482399,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 373, 'y': 487}},\n",
      "                 {'bottomright': {'x': 496, 'y': 552},\n",
      "                  'confidence': 0.41961446,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 465, 'y': 485}},\n",
      "                 {'bottomright': {'x': 511, 'y': 552},\n",
      "                  'confidence': 0.25238574,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 484, 'y': 491}},\n",
      "                 {'bottomright': {'x': 607, 'y': 550},\n",
      "                  'confidence': 0.97479993,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 580, 'y': 496}}],\n",
      " 'orange4.jpg': [{'bottomright': {'x': 351, 'y': 277},\n",
      "                  'confidence': 0.72366929,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 252, 'y': 76}},\n",
      "                 {'bottomright': {'x': 729, 'y': 289},\n",
      "                  'confidence': 0.98828381,\n",
      "                  'label': 'orange_rect',\n",
      "                  'topleft': {'x': 652, 'y': 90}}],\n",
      " 'red1.jpg': [{'bottomright': {'x': 74, 'y': 274},\n",
      "               'confidence': 0.97767258,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 0, 'y': 49}},\n",
      "              {'bottomright': {'x': 475, 'y': 288},\n",
      "               'confidence': 0.99884254,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 366, 'y': 51}},\n",
      "              {'bottomright': {'x': 799, 'y': 380},\n",
      "               'confidence': 0.26309898,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 774, 'y': 0}}],\n",
      " 'red2.jpg': [{'bottomright': {'x': 144, 'y': 354},\n",
      "               'confidence': 0.99990094,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 58, 'y': 159}},\n",
      "              {'bottomright': {'x': 466, 'y': 358},\n",
      "               'confidence': 0.99862826,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 376, 'y': 162}},\n",
      "              {'bottomright': {'x': 785, 'y': 353},\n",
      "               'confidence': 0.98753321,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 702, 'y': 181}}],\n",
      " 'red3.jpg': [{'bottomright': {'x': 490, 'y': 506},\n",
      "               'confidence': 0.97341841,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 453, 'y': 433}},\n",
      "              {'bottomright': {'x': 644, 'y': 508},\n",
      "               'confidence': 0.99873674,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 606, 'y': 431}},\n",
      "              {'bottomright': {'x': 792, 'y': 512},\n",
      "               'confidence': 0.96348494,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 753, 'y': 435}}],\n",
      " 'red4.jpg': [{'bottomright': {'x': 94, 'y': 394},\n",
      "               'confidence': 0.98795164,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 17, 'y': 229}},\n",
      "              {'bottomright': {'x': 362, 'y': 399},\n",
      "               'confidence': 0.982113,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 297, 'y': 244}},\n",
      "              {'bottomright': {'x': 643, 'y': 417},\n",
      "               'confidence': 0.99860001,\n",
      "               'label': 'red_rect',\n",
      "               'topleft': {'x': 566, 'y': 241}}]}\n"
     ]
    }
   ],
   "source": [
    "# Predictions using darkflow cythonized code and non-cythonized code\n",
    "predictions_darkflow = {}\n",
    "predictions_nondarkflow = {}\n",
    "yolo_test = YOLOTest(options)\n",
    "\n",
    "'''NOTE: The Pythonized prediction time is ~1.2 - 1.5s where as the cythonized version is < 0.5s on CPU'''\n",
    "for img_file in os.listdir(os.getcwd() + \"/sample_img\"):\n",
    "    file_path = os.getcwd() + \"/sample_img/\" + img_file\n",
    "    imgcv = cv2.imread(file_path)\n",
    "    #t = time.time()\n",
    "    result = tfnet.return_predict(imgcv)\n",
    "    #print(\"Cythonized Inference Time %.3f seconds\" % (time.time() - t))\n",
    "    predictions_darkflow[img_file] = result\n",
    "    #t = time.time()\n",
    "    result = return_predict(imgcv, yolo_test)\n",
    "    #print(\"Pythonized Inference Time %.3f seconds\" % (time.time() - t))\n",
    "    predictions_nondarkflow[img_file] = result\n",
    "\n",
    "pprint(predictions_darkflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting previous annotations directory\n"
     ]
    }
   ],
   "source": [
    "# Specify the image size to display(in inches)\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "# Function to visualize predictions on an image and store\n",
    "def visualizePredictions(predictions, source=\"darkflow\"):\n",
    "    # For each of the annnotations result plot the annotation box and display the image\n",
    "    for img_file, results in predictions.items():\n",
    "        # Read the image file\n",
    "        image_file_path = os.getcwd() + \"/sample_img/\" + img_file\n",
    "        image = cv2.imread(image_file_path)\n",
    "\n",
    "        for result in results:\n",
    "            # Get the top left co-ordinates and insert into a tuple\n",
    "            x = result['topleft']['x']\n",
    "            y = result['topleft']['y']\n",
    "            top_left = (x, y)\n",
    "\n",
    "            # Get the top left co-ordinates and insert into a tuple\n",
    "            x = result['bottomright']['x']\n",
    "            y = result['bottomright']['y']\n",
    "            bottom_right = (x, y)\n",
    "\n",
    "            # Get the label and the confidence scores\n",
    "            label = result['label']\n",
    "            confidence = result['confidence']\n",
    "\n",
    "            # Add the bounding boxes and the label with confidence scores if it is above 75%\n",
    "            if confidence > 0.70:\n",
    "                if label == \"red_rect\":\n",
    "                    label_to_display = \"Red\"\n",
    "                    color_to_display = (0, 0, 255)\n",
    "                elif label == \"orange_rect\":\n",
    "                    label_to_display = \"Yellow\"\n",
    "                    color_to_display = (0, 255, 255)\n",
    "                elif label == \"green_rect\":\n",
    "                    label_to_display = \"Green\"\n",
    "                    color_to_display = (0, 255, 0)\n",
    "\n",
    "                # Add the rectangle \n",
    "                image = cv2.rectangle(image, top_left, bottom_right, color_to_display, 3)\n",
    "\n",
    "                # Bottom left of text\n",
    "                bottom_left = (int(image.shape[0]/2) + 20, 20)\n",
    "\n",
    "                # Add the label\n",
    "                image = cv2.putText(image, label_to_display, bottom_left, \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, color_to_display, 1)\n",
    "\n",
    "        # Write the final file with or without annotations\n",
    "        cv2.imwrite(annotations_dir + \"/\" + source + \"_\" + img_file, image)\n",
    "        \n",
    "# Remove existing annotations and create new ones based on the results\n",
    "annotations_dir = os.getcwd() + \"/sample_img_annotated\"\n",
    "if os.path.exists(annotations_dir):\n",
    "    print(\"Deleting previous annotations directory\")\n",
    "    shutil.rmtree(annotations_dir)\n",
    "os.makedirs(annotations_dir)\n",
    "\n",
    "# Visualize predictions from the cythonized version and the non-cythonized version\n",
    "visualizePredictions(predictions_darkflow, \"darkflow\")\n",
    "visualizePredictions(predictions_nondarkflow, \"nodarkflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
